{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: wheel in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (60.2.0)\n",
      "Requirement already satisfied: numpy in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /cluster/home/taheeraa/.local/lib/python3.9/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/apps/eb/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    for data in train_loader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        _, outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract features using the encoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Define the classification head\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(128, 10)  # Adjust the output size for your classification task\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset again for classification\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# Combine the encoder and classifier\n",
    "classifier = Classifier()\n",
    "optimizer_classifier = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "# Lists to store training history\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    " \n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    " \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        optimizer_classifier.zero_grad()\n",
    "        encoded = encoder(inputs)\n",
    "        outputs = classifier(encoded)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_classifier.step()\n",
    " \n",
    "        # Calculate training loss\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    " \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    " \n",
    "    print(f'Epoch [{epoch + 1}/10], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    " \n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 11), train_loss_history, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), train_acc_history, label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    " \n",
    "# Now, you can use the combined model (encoder + classifier) for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the modified ResNet model for MNIST\n",
    "class MNISTResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MNISTResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, \n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = Compose([Resize((224, 224)), ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the modified ResNet model\n",
    "model = MNISTResNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lists to store training history\n",
    "train_loss_history = []\n",
    "train_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    print('Training mode')\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print('D')\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100.0 * correct / total\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/10], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "# Plot training loss and accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 11), train_loss_history, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), train_acc_history, label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now, you can use the trained model for MNIST classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
